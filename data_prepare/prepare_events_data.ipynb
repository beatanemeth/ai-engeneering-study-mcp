{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define relative paths\n",
    "INPUT_FILE_EVENTS = \"../data/wix_events_data.json\"\n",
    "OUTPUT_FOLDER = \"../data_prepared/\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, \"events_intermediate.json\")\n",
    "\n",
    "# Simple check to verify the file is where we think it is\n",
    "if os.path.exists(INPUT_FILE_EVENTS):\n",
    "    print(f\"‚úÖ Setup complete. Input file found: {INPUT_FILE_EVENTS}\")\n",
    "else:\n",
    "    print(f\"‚ùå Warning: Input file NOT found at {INPUT_FILE_EVENTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Directory Check\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "    print(f\"Created folder: {OUTPUT_FOLDER}\")\n",
    "\n",
    "# Load the raw data and FLATTEN it\n",
    "try:\n",
    "    with open(INPUT_FILE_EVENTS, 'r', encoding='utf-8') as f:\n",
    "        # Load the raw JSON as a Python list\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Flatten the nested structure (location.type, etc.)\n",
    "    events_df = pd.json_normalize(raw_data)\n",
    "    \n",
    "    print(f\"Successfully loaded and flattened {len(events_df)} records.\")\n",
    "    \n",
    "    print(\"\\n--- First 3 rows of flattened data ---\")\n",
    "    display(events_df[[\"categories.categories\"]].head(3)) \n",
    "    \n",
    "    # These columns will now look like 'location.type', 'dateAndTimeSettings.startDate', etc.\n",
    "    print(\"\\nAvailable flattened columns (first 10):\", *events_df.columns, sep=\"\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {INPUT_FILE_EVENTS} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Logic - Part-1\n",
    "\n",
    "# We don't need final_events_df = pd.DataFrame() first\n",
    "final_events_df = events_df.assign(\n",
    "    id=events_df['_id'],\n",
    "    title=events_df['title'],\n",
    "    location=events_df.get('location.type'),\n",
    "    url=events_df['eventPageUrl'],\n",
    "\n",
    "    # Standardize the date to string format\n",
    "    date=pd.to_datetime(events_df['dateAndTimeSettings.startDate']).dt.date.astype(str),\n",
    "\n",
    "    # Group Guests into a single nested object\n",
    "    # .fillna(0) ensures we don't put 'nan' into our final dictionary\n",
    "    eventGuests=events_df.fillna(0).apply(lambda row: {\n",
    "        'total': int(row.get('summaries.rsvps.totalCount', 0)),\n",
    "        'going': int(row.get('summaries.rsvps.yesCount', 0)),\n",
    "        'notGoing': int(row.get('summaries.rsvps.noCount', 0)),\n",
    "        'waitlist': int(row.get('summaries.rsvps.waitlistCount', 0)),\n",
    "    }, axis=1)\n",
    ")\n",
    "\n",
    "# Reorder and keep only necessary columns\n",
    "final_columns = ['id', 'title', 'date', 'location', 'eventGuests', 'url']\n",
    "final_events_df = final_events_df[final_columns]\n",
    "\n",
    "print(f\"‚úÖ Transformed {len(final_events_df)} events.\")\n",
    "display(final_events_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Logic - Part-2\n",
    "\n",
    "# Extract category names\n",
    "# We use .get() and check if 'val' is a list to prevent crashes on empty/NaN rows\n",
    "final_events_df['categories'] = events_df['categories.categories'].apply(\n",
    "    lambda val: [item.get('name') for item in val] if isinstance(val, list) else []\n",
    ")\n",
    "\n",
    "# Final selection and Reorder (including the new categories column)\n",
    "final_columns = ['id', 'title', 'date', 'location', 'categories', 'eventGuests', 'url']\n",
    "final_events_df = final_events_df[final_columns]\n",
    "\n",
    "print(\"‚úÖ Category extraction complete.\")\n",
    "display(final_events_df[['title', 'categories']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Logic - Part-3\n",
    "\n",
    "# Clean categories\n",
    "# Define the string to remove\n",
    "TAG_TO_REMOVE = 'publish'\n",
    "\n",
    "def clean_and_join_categories(cat_list):\n",
    "    # Ensure we are working with a list; handle None/NaN\n",
    "    if not isinstance(cat_list, list):\n",
    "        return \"\"\n",
    "    \n",
    "    # Filter out 'publish' and any None values\n",
    "    filtered_list = [c for c in cat_list if c and c != TAG_TO_REMOVE]\n",
    "    \n",
    "    # Join into a single string (e.g., \"egy√©b, marketing\")\n",
    "    return \", \".join(filtered_list)\n",
    "\n",
    "# Apply the transformation\n",
    "final_events_df['categories'] = final_events_df['categories'].apply(clean_and_join_categories)\n",
    "\n",
    "print(\"‚úÖ 'publish' removed and categories converted to string.\")\n",
    "display(final_events_df[['title', 'categories']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Logic - Part-4\n",
    "\n",
    "def extract_all_text(item):\n",
    "    \"\"\"\n",
    "    Goes through every dictionary and list inside the JSON \n",
    "    until it finds 'textData'. No matter how deep it is.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "\n",
    "    # If it's a list, check every item in the list\n",
    "    if isinstance(item, list):\n",
    "        for sub_item in item:\n",
    "            text_parts.append(extract_all_text(sub_item))\n",
    "\n",
    "    # If it's a dictionary, look for textData OR more nodes\n",
    "    elif isinstance(item, dict):\n",
    "        # FOUND THE GOAL: Grab the text\n",
    "        if 'textData' in item and 'text' in item['textData']:\n",
    "            text_parts.append(item['textData']['text'])\n",
    "        \n",
    "        # ADD NEWLINES: If it's a paragraph or list item, add a break\n",
    "        if item.get('type') in ['PARAGRAPH', 'LIST_ITEM', 'HEADING']:\n",
    "            text_parts.append(\"\\n\")\n",
    "\n",
    "        # DRILL DEEPER: Check every key in the dictionary for more lists/dicts\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, (list, dict)):\n",
    "                text_parts.append(extract_all_text(value))\n",
    "\n",
    "    return \"\".join(text_parts)\n",
    "\n",
    "def clean_description(raw_nodes):\n",
    "    \"\"\"Entry point for the DataFrame apply.\"\"\"\n",
    "    if not raw_nodes:\n",
    "        return \"\"\n",
    "    full_text = extract_all_text(raw_nodes)\n",
    "    \n",
    "    # Clean up formatting: \n",
    "    # Fix multiple newlines\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', full_text)\n",
    "    # Fix spaces that might have been added between bold/normal text\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply to your confirmed column \n",
    "final_events_df['descriptionText'] = events_df['description.nodes'].apply(clean_description)\n",
    "\n",
    "print(\"‚úÖ Description extraction should now capture the lists and nested paragraphs!\")\n",
    "display(final_events_df[['title', 'descriptionText']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update final columns to include your new description\n",
    "final_columns = ['id', 'title', 'date', 'location', 'categories', 'eventGuests', 'descriptionText', 'url']\n",
    "final_events_df = final_events_df[final_columns]\n",
    "\n",
    "print(\"‚úÖ Description reconstructed.\")\n",
    "display(final_events_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON (keeping Hungarian characters safe with ensure_ascii=False)\n",
    "final_events_df.to_json(OUTPUT_FILE, orient='records', force_ascii=False, indent=4)\n",
    "print(f\"üöÄ Data successfully exported to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Engineering)",
   "language": "python",
   "name": "ai-engeneering-study-mcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
